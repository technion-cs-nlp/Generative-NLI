# -*- coding: utf-8 -*-
"""Copy of Leveraging Pre-trained Checkpoints for Encoder-Decoder Models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OfZya_7E6RihmR3Il9C0YE8MNIIzLSQe

### **Data Preprocessing**

In this section, we show how the data can be pre-processed for training. More importantly, we try to give the reader some insight into the process of deciding how to preprocess the data.

We will need datasets and transformers to be installed.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install datasets==1.0.2
# !pip install transformers

import datasets

train_data2 = datasets.load_dataset("snli",split="train")

"""Alright, let's get a first impression of the dataset.
Alternatively, the dataset can also be visualized using the awesome [datasets viewer](https://huggingface.co/nlp/viewer/?dataset=cnn_dailymail&config=3.0.0) online.
"""

train_data2.info.description

import pandas as pd
from IPython.display import display, HTML
from datasets import ClassLabel

df = pd.DataFrame(train_data2[:1])
# del df["id"]
for column, typ in train_data2.features.items():
      if isinstance(typ, ClassLabel):
          df[column] = df[column].transform(lambda i: typ.names[i])
display(HTML(df.to_html()))

from transformers import BertTokenizerFast
tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")

# map article and summary len to dict as well as if sample is longer than 512 tokens
def map_to_length(x):
  x["hypothesis_len"] = len(tokenizer(x["hypothesis"]).input_ids)
  x["hypothesis_longer_32"] = int(x["hypothesis_len"] > 32)
  x["premise_len"] = len(tokenizer(x["premise"]).input_ids)
  x["premise_longer_16"] = int(x["premise_len"] > 16)
  x["premise_longer_32"] = int(x["premise_len"] > 32)
  return x

"""It should be sufficient to look at the first 10000 samples. We can speed up the mapping by using multiple processes with `num_proc=4`."""

sample_size = 500000
data_stats = train_data2.select(range(sample_size)).map(map_to_length, num_proc=4)

"""Having computed the length for the first 10000 samples, we should now average them together. For this, we can make use of the `.map()` function with `batched=True` and `batch_size=-1` to have access to all 10000 samples within the `.map()` function."""

def compute_and_print_stats(x):
  if len(x["premise_len"]) == sample_size:
    print(
        "hypothesis Mean: {}, %-hypotheses > 32:{}, premise Mean:{}, %-premise > 16:{}, %-premise > 32:{}".format(
            sum(x["hypothesis_len"]) / sample_size,
            sum(x["hypothesis_longer_32"]) / sample_size, 
            sum(x["premise_len"]) / sample_size,
            sum(x["premise_longer_16"]) / sample_size,
            sum(x["premise_longer_32"]) / sample_size,
        )
    )

output = data_stats.map(
  compute_and_print_stats, 
  batched=True,
  batch_size=-1,
)

labels_tokens = ['contradiction', 'entailment', 'neutral']

encoder_max_length=32
decoder_max_length=64

def process_data_to_model_inputs(batch):
  # tokenize the inputs and labels
  H_y = [batch["hypothesis"][i]+ " "+labels_tokens[batch['label'][i]] for i in range(len(batch['hypothesis']))]
  inputs = tokenizer(H_y, padding="max_length", truncation=True, max_length=encoder_max_length)
  outputs = tokenizer(batch["premise"], padding="max_length", truncation=True, max_length=decoder_max_length)

  batch["input_ids"] = inputs.input_ids
  batch["attention_mask"] = inputs.attention_mask
  batch["decoder_input_ids"] = outputs.input_ids
  batch["decoder_attention_mask"] = outputs.attention_mask
  batch["labels"] = outputs.input_ids.copy()

  # because BERT automatically shifts the labels, the labels correspond exactly to `decoder_input_ids`. 
  # We have to make sure that the PAD token is ignored
  batch["labels"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch["labels"]]

  return batch

"""Alright, let's prepare the training data."""

# batch_size = 16
batch_size = 64

train_data = train_data2.map(
    process_data_to_model_inputs, 
    batched=True, 
    batch_size=batch_size, 
    remove_columns=["premise", "hypothesis", "label"]
)

train_data

"""So far, the data was manipulated using Python's `List` format. Let's convert the data to PyTorch Tensors to be trained on GPU."""

train_data.set_format(
    type="torch", columns=["input_ids", "attention_mask", "decoder_input_ids", "decoder_attention_mask", "labels"],
)

val_data = datasets.load_dataset("snli", split="validation")

"""the mapping function is applied,"""

val_data = val_data.map(
    process_data_to_model_inputs, 
    batched=True, 
    batch_size=batch_size, 
    remove_columns=["premise", "hypothesis", "label"]
)

"""and, finally, the validation data is also converted to PyTorch tensors."""

val_data.set_format(
    type="torch", columns=["input_ids", "attention_mask", "decoder_input_ids", "decoder_attention_mask", "labels"],
)

"""Great! Now we can move to warm-starting the `EncoderDecoderModel`.

### **Warm-starting the Encoder-Decoder Model**

This section explains how an Encoder-Decoder model can be warm-started using the `bert-base-cased` checkpoint.

Let's start by importing the `EncoderDecoderModel`. For more detailed information about the `EncoderDecoderModel` class, the reader is advised to take a look at the [documentation](https://huggingface.co/transformers/model_doc/encoderdecoder.html).
"""

from transformers import EncoderDecoderModel

bert2bert = EncoderDecoderModel.from_encoder_decoder_pretrained("bert-base-uncased", "bert-base-uncased")

"""We have warm-started a `bert2bert` model, but we have not defined all the relevant parameters used for beam search decoding yet.

Let's start by setting the special tokens.
`bert-base-cased` does not have a `decoder_start_token_id` or `eos_token_id`, so we will use its `cls_token_id` and `sep_token_id` respectively. 
Also, we should define a `pad_token_id` on the config and make sure the correct `vocab_size` is set.
"""

bert2bert.config.decoder_start_token_id = tokenizer.cls_token_id
bert2bert.config.eos_token_id = tokenizer.sep_token_id
bert2bert.config.pad_token_id = tokenizer.pad_token_id
bert2bert.config.vocab_size = bert2bert.config.encoder.vocab_size

bert2bert.config.max_length = 64
bert2bert.config.min_length = 8
bert2bert.config.no_repeat_ngram_size = 3
bert2bert.config.early_stopping = True
bert2bert.config.length_penalty = 2.0
bert2bert.config.num_beams = 4

"""Alright, let's now start fine-tuning the warm-started *BERT2BERT* model.

### **Fine-Tuning Warm-Started Encoder-Decoder Models**

In this section, we will show how one can make use of the `Seq2SeqTrainer` that can be found under [examples/seq2seq/seq2seq_trainer.py](https://github.com/huggingface/transformers/blob/master/examples/seq2seq/seq2seq_trainer.py) to fine-tune a warm-started encoder-decoder model.

Let's first download the `Seq2SeqTrainer` and its training arguments `Seq2SeqTrainingArguments`.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !rm seq2seq_trainer.py
# !rm seq2seq_training_args.py
# !wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/seq2seq/seq2seq_trainer.py
# !wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/seq2seq/seq2seq_training_args.py

"""In addition, we need a couple of python packages to make the `Seq2SeqTrainer` work."""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install git-python==1.0.3
# !pip install rouge_score
# !pip install sacrebleu

"""Alright, let's import the `Seq2SeqTrainer` and the `Seq2SeqTrainingArguments`."""

from seq2seq_trainer import Seq2SeqTrainer
from seq2seq_training_args import Seq2SeqTrainingArguments

"""The `Seq2SeqTrainer` extends ðŸ¤—Transformer's Trainer for encoder-decoder models.
In short, it allows using the `generate(...)` function during evaluation, which is necessary to validate the performance of encoder-decoder models on most *sequence-to-sequence* tasks, such as *summarization*. 

For more information on the `Trainer`, one should read through [this](https://huggingface.co/transformers/training.html#trainer) short tutorial.

Let's begin by configuring the `Seq2SeqTrainingArguments`.

The argument `predict_with_generate` should be set to `True`, so that the `Seq2SeqTrainer` runs the `generate(...)` on the validation data and passes the generated output as `predictions` to the `compute_metric(...)` function which we will define later.
The additional arguments are derived from `TrainingArguments` and can be read upon [here](https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments).
For a complete training run, one should change those arguments as needed. Good default values are commented out below.

For more information on the `Seq2SeqTrainer`, the reader is advised to take a look at the [code](https://github.com/huggingface/transformers/blob/master/examples/seq2seq/seq2seq_trainer.py).
"""

training_args = Seq2SeqTrainingArguments(
    predict_with_generate=True,
    evaluation_strategy="steps",
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    fp16=True, 
    output_dir="./",
    # logging_steps=2,
    # save_steps=10,
    # eval_steps=4,
    logging_steps=500,
    save_steps=500,
    eval_steps=2500,
    warmup_steps=1000,
    save_total_limit=3,
)

"""Also, we need to define a function to correctly compute the ROUGE score during validation. Since we activated `predict_with_generate`, the `compute_metrics(...)` function expects `predictions` that were obtained using the `generate(...)` function. 
Like most summarization tasks, CNN/Dailymail is typically evaluated using the ROUGE score. 

Let's first load the ROUGE metric using the ðŸ¤—datasets library.
"""

rouge = datasets.load_metric("rouge")

"""Next, we will define the `compute_metrics(...)` function. The `rouge` metric computes the score from two lists of strings. Thus we decode both the `predictions` and `labels` - making sure that `-100` is correctly replaced by the `pad_token_id` and remove all special characters by setting `skip_special_tokens=True`."""

def compute_metrics(pred):
    labels_ids = pred.label_ids
    pred_ids = pred.predictions

    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    labels_ids[labels_ids == -100] = tokenizer.pad_token_id
    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)

    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=["rouge2"])["rouge2"].mid

    return {
        "rouge2_precision": round(rouge_output.precision, 4),
        "rouge2_recall": round(rouge_output.recall, 4),
        "rouge2_fmeasure": round(rouge_output.fmeasure, 4),
    }

"""Great, now we can pass all arguments to the `Seq2SeqTrainer` and start finetuning. Executing the following cell will take *ca.* 10 minutes 
â˜•.

Finetuning *BERT2BERT* on the complete *CNN/Dailymail* training data takes *ca.*  model takes *ca.* 8h on a single *TITAN RTX* GPU.
"""

# instantiate trainer
trainer = Seq2SeqTrainer(
    model=bert2bert,
    args=training_args,
    compute_metrics=compute_metrics,
    train_dataset=train_data,
    eval_dataset=val_data,
)
trainer.train()

"""Finally, we can load the checkpoint as usual via the `EncoderDecoderModel.from_pretrained(...)` method."""

dummy_bert2bert = EncoderDecoderModel.from_pretrained("./checkpoint-20")

"""### **Evaluation**

In a final step, we might want to evaluate the *BERT2BERT* model on the test data.

To start, instead of loading the dummy model, let's load a *BERT2BERT* model that was finetuned on the full training dataset. Also, we load its tokenizer, which is just a copy of `bert-base-cased`'s tokenizer.
"""

from transformers import BertTokenizer

bert2bert = EncoderDecoderModel.from_pretrained("patrickvonplaten/bert2bert_cnn_daily_mail").to("cuda")
tokenizer = BertTokenizer.from_pretrained("patrickvonplaten/bert2bert_cnn_daily_mail")

"""Next, we load just 2% of *CNN/Dailymail's* test data. For the full evaluation, one should obviously use 100% of the data."""

test_data = datasets.load_dataset("cnn_dailymail", "3.0.0", split="test[:2%]")

"""Now, we can again leverage ðŸ¤—dataset's handy `map()` function to generate a summary for each test sample.

For each data sample we:

- first, tokenize the `"article"`,
- second, generate the output token ids, and
- third, decode the output token ids to obtain our predicted summary.
"""

def generate_summary(batch):
    # cut off at BERT max length 512
    inputs = tokenizer(batch["article"], padding="max_length", truncation=True, max_length=512, return_tensors="pt")
    input_ids = inputs.input_ids.to("cuda")
    attention_mask = inputs.attention_mask.to("cuda")

    outputs = bert2bert.generate(input_ids, attention_mask=attention_mask)

    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)

    batch["pred_summary"] = output_str

    return batch

"""Let's run the map function to obtain the *results* dictionary that has the model's predicted summary stored for each sample. Executing the following cell may take *ca.* 10min â˜•."""

batch_size = 16  # change to 64 for full evaluation

results = test_data.map(generate_summary, batched=True, batch_size=batch_size, remove_columns=["article"])

"""Finally, we compute the ROUGE score."""

rouge.compute(predictions=results["pred_summary"], references=results["highlights"], rouge_types=["rouge2"])["rouge2"].mid

"""That's it. We've shown how to warm-start a *BERT2BERT* model and fine-tune/evaluate it on the CNN/Dailymail dataset.

The fully trained *BERT2BERT* model is uploaded to the ðŸ¤—model hub under [patrickvonplaten/bert2bert_cnn_daily_mail](https://huggingface.co/patrickvonplaten/bert2bert_cnn_daily_mail). 

The model achieves a ROUGE-2 score of **18.22** on the full evaluation data, which is even a little better than reported in the paper.

For some summarization examples, the reader is advised to use the online inference API of the model, [here](https://huggingface.co/patrickvonplaten/bert2bert_cnn_daily_mail).

Thanks a lot to Sascha Rothe, Shashi Narayan, and Aliaksei Severyn from Google Research, and Victor Sanh, Sylvain Gugger, and Thomas Wolf from ðŸ¤—Hugging Face for proof-reading and giving very much appreciated feedback.
"""