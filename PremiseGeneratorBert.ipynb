{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PremiseGeneratorBert.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyR_zKp7sg9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLVmonse5mir",
        "colab_type": "code",
        "outputId": "0b426772-420c-4fd0-d914-8a4814e159a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd drive/My\\ Drive/PremiseGeneratorBert"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/PremiseGeneratorBert\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ab28ETS6XIM",
        "colab_type": "code",
        "outputId": "81e38e87-b2e9-45c2-986c-a4f13009613f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "!pip install transformers\n",
        "!pip install py-rouge nltk"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/fc/bd726a15ab2c66dc09306689d04da07a3770dad724f0883f0a4bfb745087/transformers-2.4.1-py3-none-any.whl (475kB)\n",
            "\u001b[K     |████████████████████████████████| 481kB 2.9MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.0.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/36/7af38d572c935f8e0462ec7b4f7a46d73a2b3b1a938f50a5e8132d5b2dc5/tokenizers-0.0.11-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 59.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 53.2MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 46.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=5115ba2a24365fea1f4e355d9b2a2a38d53c15238ba9539e818d892938190dbf\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.0.11 transformers-2.4.1\n",
            "Collecting py-rouge\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/1d/0bdbaf559fb7afe32308ebc84a2028600988212d7eb7fb9f69c4e829e4a0/py_rouge-1.1-py3-none-any.whl (56kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Installing collected packages: py-rouge\n",
            "Successfully installed py-rouge-1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10jkWpZK9ywv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %pycat /usr/local/lib/python3.6/dist-packages/transformers/modeling_encoder_decoder.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm_p6T1aB1ax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm /usr/local/lib/python3.6/dist-packages/transformers/modeling_encoder_decoder.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHiHrGtCAs4Q",
        "colab_type": "code",
        "outputId": "7336ff5e-f830-43f8-f10c-851c4e23cabb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile /usr/local/lib/python3.6/dist-packages/transformers/modeling_encoder_decoder.py\n",
        "# coding=utf-8\n",
        "# Copyright 2018 The HuggingFace Inc. team.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\" Classes to support Encoder-Decoder architectures \"\"\"\n",
        "\n",
        "\n",
        "import logging\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from .modeling_auto import AutoModel, AutoModelWithLMHead\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class PreTrainedEncoderDecoder(nn.Module):\n",
        "    r\"\"\"\n",
        "        :class:`~transformers.PreTrainedEncoderDecoder` is a generic model class that will be\n",
        "        instantiated as a transformer architecture with one of the base model\n",
        "        classes of the library as encoder and (optionally) another one as\n",
        "        decoder when created with the `AutoModel.from_pretrained(pretrained_model_name_or_path)`\n",
        "        class method.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(\n",
        "        cls,\n",
        "        encoder_pretrained_model_name_or_path=None,\n",
        "        decoder_pretrained_model_name_or_path=None,\n",
        "        *model_args,\n",
        "        **kwargs\n",
        "    ):\n",
        "        r\"\"\" Instantiates an encoder and a decoder from one or two base classes of the library from pre-trained model checkpoints.\n",
        "\n",
        "\n",
        "        The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated)\n",
        "        To train the model, you need to first set it back in training mode with `model.train()`\n",
        "\n",
        "        Params:\n",
        "            encoder_pretrained_model_name_or_path: information necessary to initiate the encoder. Either:\n",
        "\n",
        "                - a string with the `shortcut name` of a pre-trained model to load from cache or download, e.g.: ``bert-base-uncased``.\n",
        "                - a string with the `identifier name` of a pre-trained model that was user-uploaded to our S3, e.g.: ``dbmdz/bert-base-german-cased``.\n",
        "                - a path to a `directory` containing model weights saved using :func:`~transformers.PreTrainedModel.save_pretrained`, e.g.: ``./my_model_directory/encoder``.\n",
        "                - a path or url to a `tensorflow index checkpoint file` (e.g. `./tf_model/model.ckpt.index`). In this case, ``from_tf`` should be set to True and a configuration object should be provided as ``config`` argument. This loading path is slower than converting the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n",
        "\n",
        "            decoder_pretrained_model_name_or_path: information necessary to initiate the decoder. Either:\n",
        "\n",
        "                - a string with the `shortcut name` of a pre-trained model to load from cache or download, e.g.: ``bert-base-uncased``.\n",
        "                - a string with the `identifier name` of a pre-trained model that was user-uploaded to our S3, e.g.: ``dbmdz/bert-base-german-cased``.\n",
        "                - a path to a `directory` containing model weights saved using :func:`~transformers.PreTrainedModel.save_pretrained`, e.g.: ``./my_model_directory/decoder``.\n",
        "                - a path or url to a `tensorflow index checkpoint file` (e.g. `./tf_model/model.ckpt.index`). In this case, ``from_tf`` should be set to True and a configuration object should be provided as ``config`` argument. This loading path is slower than converting the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n",
        "\n",
        "            model_args: (`optional`) Sequence of positional arguments:\n",
        "                All remaning positional arguments will be passed to the underlying model's ``__init__`` method\n",
        "\n",
        "            config: (`optional`) instance of a class derived from :class:`~transformers.PretrainedConfig`:\n",
        "                Configuration for the model to use instead of an automatically loaded configuation. Configuration can be automatically loaded when:\n",
        "\n",
        "                - the model is a model provided by the library (loaded with the ``shortcut-name`` string of a pretrained model), or\n",
        "                - the model was saved using :func:`~transformers.PreTrainedModel.save_pretrained` and is reloaded by suppling the save directory.\n",
        "                - the model is loaded by suppling a local directory as ``pretrained_model_name_or_path`` and a configuration JSON file named `config.json` is found in the directory.\n",
        "\n",
        "            state_dict: (`optional`) dict:\n",
        "                an optional state dictionnary for the model to use instead of a state dictionary loaded from saved weights file.\n",
        "                This option can be used if you want to create a model from a pretrained configuration but load your own weights.\n",
        "                In this case though, you should check if using :func:`~transformers.PreTrainedModel.save_pretrained` and :func:`~transformers.PreTrainedModel.from_pretrained` is not a simpler option.\n",
        "\n",
        "            cache_dir: (`optional`) string:\n",
        "                Path to a directory in which a downloaded pre-trained model\n",
        "                configuration should be cached if the standard cache should not be used.\n",
        "\n",
        "            force_download: (`optional`) boolean, default False:\n",
        "                Force to (re-)download the model weights and configuration files and override the cached versions if they exists.\n",
        "\n",
        "            proxies: (`optional`) dict, default None:\n",
        "                A dictionary of proxy servers to use by protocol or endpoint, e.g.: {'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}.\n",
        "                The proxies are used on each request.\n",
        "\n",
        "            output_loading_info: (`optional`) boolean:\n",
        "                Set to ``True`` to also return a dictionnary containing missing keys, unexpected keys and error messages.\n",
        "\n",
        "            kwargs: (`optional`) Remaining dictionary of keyword arguments.\n",
        "                Can be used to update the configuration object (after it being loaded) and initiate the model. (e.g. ``output_attention=True``). Behave differently depending on whether a `config` is provided or automatically loaded:\n",
        "\n",
        "                - If a configuration is provided with ``config``, ``**kwargs`` will be directly passed to the underlying model's ``__init__`` method (we assume all relevant updates to the configuration have already been done)\n",
        "                - If a configuration is not provided, ``kwargs`` will be first passed to the configuration class initialization function (:func:`~transformers.PretrainedConfig.from_pretrained`). Each key of ``kwargs`` that corresponds to a configuration attribute will be used to override said attribute with the supplied ``kwargs`` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's ``__init__`` function.\n",
        "\n",
        "                You can specify kwargs sepcific for the encoder and decoder by prefixing the key with `encoder_` and `decoder_` respectively. (e.g. ``decoder_output_attention=True``). The remaining kwargs will be passed to both encoders and decoders.\n",
        "\n",
        "        Examples::\n",
        "\n",
        "            # For example purposes. Not runnable.\n",
        "            model = PreTrainedEncoderDecoder.from_pretained('bert-base-uncased', 'bert-base-uncased') # initialize Bert2Bert\n",
        "        \"\"\"\n",
        "\n",
        "        # keyword arguments come in 3 flavors: encoder-specific (prefixed by\n",
        "        # `encoder_`), decoder-specific (prefixed by `decoder_`) and those\n",
        "        # that apply to the model as a whole.\n",
        "        # We let the specific kwargs override the common ones in case of conflict.\n",
        "        kwargs_common = {\n",
        "            argument: value\n",
        "            for argument, value in kwargs.items()\n",
        "            if not argument.startswith(\"encoder_\") and not argument.startswith(\"decoder_\")\n",
        "        }\n",
        "        kwargs_decoder = kwargs_common.copy()\n",
        "        kwargs_encoder = kwargs_common.copy()\n",
        "        kwargs_encoder.update(\n",
        "            {\n",
        "                argument[len(\"encoder_\") :]: value\n",
        "                for argument, value in kwargs.items()\n",
        "                if argument.startswith(\"encoder_\")\n",
        "            }\n",
        "        )\n",
        "        kwargs_decoder.update(\n",
        "            {\n",
        "                argument[len(\"decoder_\") :]: value\n",
        "                for argument, value in kwargs.items()\n",
        "                if argument.startswith(\"decoder_\")\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Load and initialize the encoder and decoder\n",
        "        # The distinction between encoder and decoder at the model level is made\n",
        "        # by the value of the flag `is_decoder` that we need to set correctly.\n",
        "        encoder = kwargs_encoder.pop(\"model\", None)\n",
        "        if encoder is None:\n",
        "            encoder = AutoModel.from_pretrained(encoder_pretrained_model_name_or_path, *model_args, **kwargs_encoder)\n",
        "        encoder.config.is_decoder = False\n",
        "\n",
        "        decoder = kwargs_decoder.pop(\"model\", None)\n",
        "        if decoder is None:\n",
        "            decoder = AutoModelWithLMHead.from_pretrained(decoder_pretrained_model_name_or_path, **kwargs_decoder)\n",
        "        decoder.config.is_decoder = True\n",
        "\n",
        "        model = cls(encoder, decoder)\n",
        "\n",
        "        return model\n",
        "\n",
        "    def save_pretrained(self, save_directory):\n",
        "        \"\"\" Save a Seq2Seq model and its configuration file in a format such\n",
        "        that it can be loaded using `:func:`~transformers.PreTrainedEncoderDecoder.from_pretrained`\n",
        "\n",
        "        We save the encoder' and decoder's parameters in two separate directories.\n",
        "        \"\"\"\n",
        "\n",
        "        # If the root output directory does not exist, create it\n",
        "        if not os.path.exists(save_directory):\n",
        "            os.mkdir(save_directory)\n",
        "\n",
        "        # Check whether the output directory is empty or not\n",
        "        sub_directories = [\n",
        "            directory\n",
        "            for directory in os.listdir(save_directory)\n",
        "            if os.path.isdir(os.path.join(save_directory, directory))\n",
        "        ]\n",
        "\n",
        "        if len(sub_directories) > 0:\n",
        "            if \"encoder\" in sub_directories and \"decoder\" in sub_directories:\n",
        "                print(\n",
        "                    \"WARNING: there is an older version of encoder-decoder saved in\"\n",
        "                    + \" the output directory. The default behaviour is to overwrite them.\"\n",
        "                )\n",
        "\n",
        "            # Empty the output directory\n",
        "            for directory_to_remove in sub_directories:\n",
        "                # Remove all files into the subdirectory\n",
        "                files_to_remove = os.listdir(os.path.join(save_directory, directory_to_remove))\n",
        "                for file_to_remove in files_to_remove:\n",
        "                    os.remove(os.path.join(save_directory, directory_to_remove, file_to_remove))\n",
        "                # Remove the subdirectory itself\n",
        "                os.rmdir(os.path.join(save_directory, directory_to_remove))\n",
        "\n",
        "            assert len(os.listdir(save_directory)) == 0  # sanity check\n",
        "\n",
        "        # Create the \"encoder\" directory inside the output directory and save the encoder into it\n",
        "        if not os.path.exists(os.path.join(save_directory, \"encoder\")):\n",
        "            os.mkdir(os.path.join(save_directory, \"encoder\"))\n",
        "        self.encoder.save_pretrained(os.path.join(save_directory, \"encoder\"))\n",
        "\n",
        "        # Create the \"encoder\" directory inside the output directory and save the decoder into it\n",
        "        if not os.path.exists(os.path.join(save_directory, \"decoder\")):\n",
        "            os.mkdir(os.path.join(save_directory, \"decoder\"))\n",
        "        self.decoder.save_pretrained(os.path.join(save_directory, \"decoder\"))\n",
        "\n",
        "    def forward(self, encoder_input_ids, decoder_input_ids, **kwargs):\n",
        "        \"\"\" The forward pass on a seq2eq depends what we are performing:\n",
        "\n",
        "        - During training we perform one forward pass through both the encoder\n",
        "          and decoder;\n",
        "        - During prediction, we perform one forward pass through the encoder,\n",
        "          and then perform several forward passes with the encoder's hidden\n",
        "          state through the decoder to decode a full sequence.\n",
        "\n",
        "        Therefore, we skip the forward pass on the encoder if an argument named\n",
        "        `encoder_hidden_state` is passed to this function.\n",
        "\n",
        "        Params:\n",
        "            encoder_input_ids: ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``\n",
        "                Indices of encoder input sequence tokens in the vocabulary.\n",
        "            decoder_input_ids: ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``\n",
        "                Indices of decoder input sequence tokens in the vocabulary.\n",
        "            kwargs: (`optional`) Remaining dictionary of keyword arguments.\n",
        "        \"\"\"\n",
        "        kwargs_encoder, kwargs_decoder = self.prepare_model_kwargs(**kwargs)\n",
        "\n",
        "        # Encode if needed (training, first prediction pass)\n",
        "        encoder_hidden_states = kwargs_encoder.pop(\"hidden_states\", None)\n",
        "        if encoder_hidden_states is None:\n",
        "            encoder_outputs = self.encoder(encoder_input_ids, **kwargs_encoder)\n",
        "            encoder_hidden_states = encoder_outputs[0]\n",
        "        else:\n",
        "            encoder_outputs = ()\n",
        "\n",
        "        kwargs_decoder[\"encoder_hidden_states\"] = encoder_hidden_states\n",
        "        decoder_outputs = self.decoder(decoder_input_ids, **kwargs_decoder)\n",
        "\n",
        "        return decoder_outputs + encoder_outputs\n",
        "\n",
        "    @staticmethod\n",
        "    def prepare_model_kwargs(**kwargs):\n",
        "        \"\"\" Prepare the encoder and decoder's keyword arguments.\n",
        "\n",
        "        Keyword arguments come in 3 flavors:\n",
        "        - encoder-specific (prefixed by `encoder_`)\n",
        "        - decoder-specific (prefixed by `decoder_`)\n",
        "        - those that apply to the model as whole.\n",
        "\n",
        "        We let the specific kwargs override the common ones in case of\n",
        "        conflict.\n",
        "        \"\"\"\n",
        "        kwargs_common = {\n",
        "            argument: value\n",
        "            for argument, value in kwargs.items()\n",
        "            if not argument.startswith(\"encoder_\") and not argument.startswith(\"decoder_\")\n",
        "        }\n",
        "        decoder_kwargs = kwargs_common.copy()\n",
        "        encoder_kwargs = kwargs_common.copy()\n",
        "        encoder_kwargs.update(\n",
        "            {\n",
        "                argument[len(\"encoder_\") :]: value\n",
        "                for argument, value in kwargs.items()\n",
        "                if argument.startswith(\"encoder_\")\n",
        "            }\n",
        "        )\n",
        "        decoder_kwargs.update(\n",
        "            {\n",
        "                argument[len(\"decoder_\") :]: value\n",
        "                for argument, value in kwargs.items()\n",
        "                if argument.startswith(\"decoder_\")\n",
        "            }\n",
        "        )\n",
        "        decoder_kwargs[\"encoder_attention_mask\"] = encoder_kwargs.get(\"attention_mask\", None)\n",
        "        return encoder_kwargs, decoder_kwargs\n",
        "\n",
        "\n",
        "class Model2Model(PreTrainedEncoderDecoder):\n",
        "    r\"\"\"\n",
        "        :class:`~transformers.Model2Model` instantiates a Seq2Seq2 model\n",
        "        where both of the encoder and decoder are of the same family. If the\n",
        "        name of or that path to a pretrained model is specified the encoder and\n",
        "        the decoder will be initialized with the pretrained weight (the\n",
        "        cross-attention will be intialized randomly if its weights are not\n",
        "        present).\n",
        "\n",
        "        It is possible to override this behavior and initialize, say, the decoder randomly\n",
        "        by creating it beforehand as follows\n",
        "\n",
        "            config = BertConfig.from_pretrained()\n",
        "            decoder = BertForMaskedLM(config)\n",
        "            model = Model2Model.from_pretrained('bert-base-uncased', decoder_model=decoder)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.tie_weights()\n",
        "\n",
        "    def tie_weights(self):\n",
        "        \"\"\" Tying the encoder and decoders' embeddings together.\n",
        "\n",
        "       We need for each to get down to the embedding weights. However the\n",
        "        different model classes are inconsistent to that respect:\n",
        "        - BertModel: embeddings.word_embeddings\n",
        "        - RoBERTa: embeddings.word_embeddings\n",
        "        - XLMModel: embeddings\n",
        "        - GPT2: wte\n",
        "        - BertForMaskedLM: bert.embeddings.word_embeddings\n",
        "        - RobertaForMaskedLM: roberta.embeddings.word_embeddings\n",
        "\n",
        "        argument of the XEmbedding layer for each model, but it is \"blocked\"\n",
        "        by a model-specific keyword (bert, )...\n",
        "        \"\"\"\n",
        "        # self._tie_or_clone_weights(self.encoder, self.decoder)\n",
        "        pass\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, pretrained_model_name_or_path, *args, **kwargs):\n",
        "\n",
        "        if (\n",
        "            \"bert\" not in pretrained_model_name_or_path\n",
        "            or \"roberta\" in pretrained_model_name_or_path\n",
        "            or \"distilbert\" in pretrained_model_name_or_path\n",
        "        ):\n",
        "            raise ValueError(\"Only the Bert model is currently supported.\")\n",
        "\n",
        "        model = super().from_pretrained(\n",
        "            encoder_pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
        "            decoder_pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
        "            *args,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "class Model2LSTM(PreTrainedEncoderDecoder):\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, *args, **kwargs):\n",
        "        if kwargs.get(\"decoder_model\", None) is None:\n",
        "            # We will create a randomly initilized LSTM model as decoder\n",
        "            if \"decoder_config\" not in kwargs:\n",
        "                raise ValueError(\n",
        "                    \"To load an LSTM in Encoder-Decoder model, please supply either: \"\n",
        "                    \"    - a torch.nn.LSTM model as `decoder_model` parameter (`decoder_model=lstm_model`), or\"\n",
        "                    \"    - a dictionary of configuration parameters that will be used to initialize a\"\n",
        "                    \"      torch.nn.LSTM model as `decoder_config` keyword argument. \"\n",
        "                    \"      E.g. `decoder_config={'input_size': 768, 'hidden_size': 768, 'num_layers': 2}`\"\n",
        "                )\n",
        "            kwargs[\"decoder_model\"] = torch.nn.LSTM(kwargs.pop(\"decoder_config\"))\n",
        "        model = super().from_pretrained(*args, **kwargs)\n",
        "        return model"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing /usr/local/lib/python3.6/dist-packages/transformers/modeling_encoder_decoder.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d3acb21a-c512-4e9c-e795-2a04d9a0d200",
        "id": "N7fmeAz96Ktm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "!python -m main run-exp --run-name dimi --data-dir-prefix './data/snli_1.0/cl_snli' --bs-train 16 --batches 100 --bs-test 1 --model-name='bert-base-uncased' --lr 2e-5"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*** Starting run_experiment with config:\n",
            "Namespace(batches=100, bs_test=1, bs_train=16, checkpoints=None, data_dir_prefix='./data/snli_1.0/cl_snli', early_stopping=3, epochs=100, lr=2e-05, max_len=0, model_name='bert-base-uncased', out_dir='./results', reg=0.001, run_name='dimi', seed=None)\n",
            "Longest Sequence is: 54 token_ids\n",
            "Setting max_len to: 64\n",
            "Labels IDs: [30522, 30523, 30524]\n",
            "--- EPOCH 1/100 ---\n",
            "train_batch (2.071):  50% 50/100 [00:16<00:16,  3.04it/s]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9wbdG1iB9ug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}